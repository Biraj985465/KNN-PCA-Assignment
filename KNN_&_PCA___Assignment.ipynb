{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# KNN & PCA | Assignment"
      ],
      "metadata": {
        "id": "uqkMe_dF6o-Q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 1: What is K-Nearest Neighbors (KNN) and how does it work in both\n",
        "classification and regression problems?"
      ],
      "metadata": {
        "id": "Ucbvkwy96sIm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "  - K-Nearest Neighbors (KNN) is a non-parametric, instance-based machine learning algorithm that can be used for both classification and regression tasks. It makes predictions based on the similarity between a new data point and the points in the training dataset.\n",
        "\n",
        "How KNN Works:\n",
        "1. Store the training data KNN is lazy learning, meaning it doesn’t build an explicit model. It just stores the training data.\n",
        "\n",
        "2. Compute distance to neighbors When making a prediction for a new input, KNN calculates the distance between the new point and all training points.\n",
        "\n",
        "3. Common distance metrics: Euclidean, Manhattan, or Minkowski distance.\n",
        "Select K nearest neighbors\n",
        "\n",
        "Choose the K closest points from the training data based on the distance metric.\n",
        "4. Predict output\n",
        "\n",
        "    1. Classification:\n",
        "\n",
        "        I. Count the classes of the K neighbors.\n",
        "        II. Assign the class that occurs most frequently (majority voting).\n",
        "        III. Example: If 3 neighbors are \"A\" and 2 are \"B\", predict \"A\".\n",
        "\n",
        "    2. Regression:\n",
        "\n",
        "        I. Take the average (or weighted average) of the K neighbors’ target values.\n",
        "        II. Example: If neighbors’ values are 10, 12, 15 → predict (10+12+15)/3 = 12.33"
      ],
      "metadata": {
        "id": "f9TRRBdkjza8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 2: What is the Curse of Dimensionality and how does it affect KNN\n",
        "performance?"
      ],
      "metadata": {
        "id": "DrWEquLP6r-_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "  - The Curse of Dimensionality refers to the problems that arise when working with high-dimensional data—i.e., datasets with a very large number of features. It particularly affects algorithms like K-Nearest Neighbors (KNN) that rely on distance metrics.\n",
        "\n",
        "  Effects on KNN Performance:\n",
        "1.   Distance metrics lose significance\n",
        "2.   Increased overfitting\n",
        "3.   Higher computational cost\n",
        "\n"
      ],
      "metadata": {
        "id": "vSyZAOeRkfBB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 3: What is Principal Component Analysis (PCA)? How is it different from\n",
        "feature selection?"
      ],
      "metadata": {
        "id": "U0i-OT7s6r3B"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "  - Principal Component Analysis (PCA) is a dimensionality reduction technique used in machine learning and statistics. Its goal is to reduce the number of features in a dataset while retaining as much variance (information) as possible.\n",
        "\n",
        "How PCA Works\n",
        "1. **Standardize the data**\n",
        "*   Scale features so that each has mean 0 and standard deviation 1 (important if features have different scales).\n",
        "\n",
        "2. **Compute covariance matrix**\n",
        "*   Measures how features vary together.\n",
        "\n",
        "3. **Calculate eigenvectors and eigenvalues**\n",
        "*   Eigenvectors (principal components) define new axes in the feature space.\n",
        "*   Eigenvalues indicate how much variance is captured by each component.\n",
        "\n",
        "3. **Select top principal components**\n",
        "*   Keep the first k components that capture most of the variance.\n",
        "\n",
        "4. **Transform the original data**\n",
        "*   Project data onto these components, reducing dimensions while retaining essential patterns."
      ],
      "metadata": {
        "id": "t5kswY44lKNH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 4: What are eigenvalues and eigenvectors in PCA, and why are they\n",
        "important?"
      ],
      "metadata": {
        "id": "MCORX0qy6rvo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "  - In Principal Component Analysis (PCA), eigenvalues and eigenvectors are fundamental because they define the principal components, which determine how the data is transformed and reduced in dimensions.\n",
        "\n",
        "1. Eigenvectors:\n",
        "*   An eigenvector is a direction in the feature space along which the data varies the most.\n",
        "*   In PCA, eigenvectors of the covariance matrix represent the axes of the new feature space (principal components).\n",
        "*   They are unit vectors that point in the directions where the data has maximum variance.\n",
        "\n",
        "    * Intuition:\n",
        "*   If your dataset is like a cloud of points, eigenvectors are the directions along which the cloud stretches the most.\n",
        "\n",
        "2. Eigenvalues:\n",
        "*   An eigenvalue corresponds to an eigenvector and indicates how much variance is captured along that direction.\n",
        "*   Larger eigenvalues → more variance captured → more “information” along that component.\n",
        "*   PCA ranks principal components by eigenvalues to decide which components to keep.\n",
        "\n",
        "    * Intuition:\n",
        "*   Think of eigenvalue as the “importance” of the corresponding eigenvector: higher means that direction explains more of the data’s spread.\n",
        "\n",
        "  * Why They Are Important in PCA\n",
        "\n",
        "1. Determine principal components:\n",
        "\n",
        "*   Eigenvectors define the directions of the new axes (components).\n",
        "*   Eigenvalues rank these axes by importance.\n",
        "\n",
        "2. Dimensionality reduction:\n",
        "*   By keeping the eigenvectors with the largest eigenvalues, you retain most of the variance while reducing the number of features.\n",
        "\n",
        "\n",
        "3. Data transformation\n",
        "*   Projecting data onto eigenvectors (principal components) creates uncorrelated features that summarize the original data efficiently."
      ],
      "metadata": {
        "id": "MGtzQWOSl_6o"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 5: How do KNN and PCA complement each other when applied in a single\n",
        "pipeline?"
      ],
      "metadata": {
        "id": "D3eTm8c_6rp-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "  - Complementarity\n",
        "\n",
        "1. PCA reduces dimensionality\n",
        "*   The Wine dataset has 13 numeric features.\n",
        "*   PCA projects them onto fewer components that retain most of the variance, removing redundant or noisy information.\n",
        "\n",
        "2. KNN is distance-based\n",
        "*   KNN relies on distances between points.\n",
        "*   Fewer, meaningful dimensions from PCA make these distances more reliable, avoiding the curse of dimensionality.\n",
        "\n",
        "3. Efficiency and accuracy\n",
        "*   Reduced dimensions → faster KNN computations.\n",
        "*   Focused features → better neighbor selection → improved classification accuracy.\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "4VbxMEU6n0FY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Python Example: KNN + PCA on Wine Dataset\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "\n",
        "from sklearn.datasets import load_wine\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "data = load_wine()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "pipeline = Pipeline([\n",
        "    ('scaler', StandardScaler()),\n",
        "    ('pca', PCA(n_components=5)),\n",
        "    ('knn', KNeighborsClassifier(n_neighbors=5))\n",
        "])\n",
        "\n",
        "pipeline.fit(X_train, y_train)\n",
        "\n",
        "y_pred = pipeline.predict(X_test)\n",
        "\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"KNN with PCA Accuracy: {accuracy:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "227fKzKaoRZO",
        "outputId": "95a9bdff-f7d5-431c-856d-4775b6880fd7"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "KNN with PCA Accuracy: 0.9444\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "  - Explanation:\n",
        "\n",
        "StandardScaler: Standardizes features before PCA.\n",
        "PCA(n_components=5): Keeps the 5 components that capture most variance.\n",
        "KNeighborsClassifier: Runs KNN in the reduced PCA space.\n",
        "Pipeline: Ensures all steps are applied consistently to train and test data."
      ],
      "metadata": {
        "id": "LuMKQiimowdG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Dataset:\n",
        "Use the Wine Dataset from sklearn.datasets.load_wine().\n",
        "  - Question 6: Train a KNN Classifier on the Wine dataset with and without feature\n",
        "scaling. Compare model accuracy in both cases."
      ],
      "metadata": {
        "id": "iMIYbUgf6rmu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "from sklearn.datasets import load_wine\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "data = load_wine()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "knn_no_scaling = KNeighborsClassifier(n_neighbors=5)\n",
        "knn_no_scaling.fit(X_train, y_train)\n",
        "y_pred_no_scaling = knn_no_scaling.predict(X_test)\n",
        "accuracy_no_scaling = accuracy_score(y_test, y_pred_no_scaling)\n",
        "print(f\"KNN Accuracy without scaling: {accuracy_no_scaling:.4f}\")\n",
        "\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "knn_scaled = KNeighborsClassifier(n_neighbors=5)\n",
        "knn_scaled.fit(X_train_scaled, y_train)\n",
        "y_pred_scaled = knn_scaled.predict(X_test_scaled)\n",
        "accuracy_scaled = accuracy_score(y_test, y_pred_scaled)\n",
        "print(f\"KNN Accuracy with scaling: {accuracy_scaled:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rMCOPGUjo3IX",
        "outputId": "d683c1d9-0cf2-4f33-a803-a0d069768e3b"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "KNN Accuracy without scaling: 0.7222\n",
            "KNN Accuracy with scaling: 0.9444\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 7: Train a PCA model on the Wine dataset and print the explained variance\n",
        "ratio of each principal component."
      ],
      "metadata": {
        "id": "cZJJsPkv6rj3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "\n",
        "from sklearn.datasets import load_wine\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.decomposition import PCA\n",
        "\n",
        "data = load_wine()\n",
        "X = data.data\n",
        "\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "pca = PCA()\n",
        "X_pca = pca.fit_transform(X_scaled)\n",
        "\n",
        "explained_variance = pca.explained_variance_ratio_\n",
        "for i, ratio in enumerate(explained_variance, start=1):\n",
        "    print(f\"Principal Component {i}: {ratio:.4f}\")\n",
        "\n",
        "cumulative_variance = explained_variance.cumsum()\n",
        "for i, cum_var in enumerate(cumulative_variance, start=1):\n",
        "    print(f\"Cumulative variance after PC{i}: {cum_var:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TfKmcbnPpMF6",
        "outputId": "a5a6cef1-feb5-4346-dbda-63e53da1b940"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Principal Component 1: 0.3620\n",
            "Principal Component 2: 0.1921\n",
            "Principal Component 3: 0.1112\n",
            "Principal Component 4: 0.0707\n",
            "Principal Component 5: 0.0656\n",
            "Principal Component 6: 0.0494\n",
            "Principal Component 7: 0.0424\n",
            "Principal Component 8: 0.0268\n",
            "Principal Component 9: 0.0222\n",
            "Principal Component 10: 0.0193\n",
            "Principal Component 11: 0.0174\n",
            "Principal Component 12: 0.0130\n",
            "Principal Component 13: 0.0080\n",
            "Cumulative variance after PC1: 0.3620\n",
            "Cumulative variance after PC2: 0.5541\n",
            "Cumulative variance after PC3: 0.6653\n",
            "Cumulative variance after PC4: 0.7360\n",
            "Cumulative variance after PC5: 0.8016\n",
            "Cumulative variance after PC6: 0.8510\n",
            "Cumulative variance after PC7: 0.8934\n",
            "Cumulative variance after PC8: 0.9202\n",
            "Cumulative variance after PC9: 0.9424\n",
            "Cumulative variance after PC10: 0.9617\n",
            "Cumulative variance after PC11: 0.9791\n",
            "Cumulative variance after PC12: 0.9920\n",
            "Cumulative variance after PC13: 1.0000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 8: Train a KNN Classifier on the PCA-transformed dataset (retain top 2\n",
        "components). Compare the accuracy with the original dataset."
      ],
      "metadata": {
        "id": "2HEroFD46rhT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "from sklearn.datasets import load_wine\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "data = load_wine()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "knn_original = KNeighborsClassifier(n_neighbors=5)\n",
        "knn_original.fit(X_train_scaled, y_train)\n",
        "y_pred_original = knn_original.predict(X_test_scaled)\n",
        "accuracy_original = accuracy_score(y_test, y_pred_original)\n",
        "print(f\"KNN Accuracy on Original Data: {accuracy_original:.4f}\")\n",
        "\n",
        "pca = PCA(n_components=2)\n",
        "X_train_pca = pca.fit_transform(X_train_scaled)\n",
        "X_test_pca = pca.transform(X_test_scaled)\n",
        "\n",
        "knn_pca = KNeighborsClassifier(n_neighbors=5)\n",
        "knn_pca.fit(X_train_pca, y_train)\n",
        "y_pred_pca = knn_pca.predict(X_test_pca)\n",
        "accuracy_pca = accuracy_score(y_test, y_pred_pca)\n",
        "print(f\"KNN Accuracy on PCA-transformed Data (2 components): {accuracy_pca:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pAIMNQrnpYN-",
        "outputId": "5f3fcfb8-a347-4a78-a334-2c311dfeba93"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "KNN Accuracy on Original Data: 0.9444\n",
            "KNN Accuracy on PCA-transformed Data (2 components): 1.0000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 9: Train a KNN Classifier with different distance metrics (euclidean,\n",
        "manhattan) on the scaled Wine dataset and compare the results."
      ],
      "metadata": {
        "id": "T2uqSIC86re6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "from sklearn.datasets import load_wine\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "data = load_wine()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "metrics = ['euclidean', 'manhattan']\n",
        "\n",
        "for metric in metrics:\n",
        "    knn = KNeighborsClassifier(n_neighbors=5, metric=metric)\n",
        "    knn.fit(X_train_scaled, y_train)\n",
        "    y_pred = knn.predict(X_test_scaled)\n",
        "    accuracy = accuracy_score(y_test, y_pred)\n",
        "    print(f\"KNN Accuracy with {metric} distance: {accuracy:.4f}\")\n",
        ""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6jhWUbKCpo4g",
        "outputId": "8575148d-b3a6-4729-f634-3825a950ba78"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "KNN Accuracy with euclidean distance: 0.9444\n",
            "KNN Accuracy with manhattan distance: 0.9444\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 10: You are working with a high-dimensional gene expression dataset to\n",
        "classify patients with different types of cancer.\n",
        "Due to the large number of features and a small number of samples, traditional models\n",
        "overfit.\n",
        "Explain how you would:\n",
        "● Use PCA to reduce dimensionality\n",
        "● Decide how many components to keep\n",
        "● Use KNN for classification post-dimensionality reduction\n",
        "● Evaluate the model\n",
        "● Justify this pipeline to your stakeholders as a robust solution for real-world\n",
        "biomedical data"
      ],
      "metadata": {
        "id": "y5bTajqQ6rck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "  - Due to the large number of features and a small number of samples, traditional models overfit.\n",
        "\n",
        "Explain how you would:\n",
        "\n",
        "● Use PCA to reduce dimensionality\n",
        "\n",
        "● Decide how many components to keep\n",
        "\n",
        "● Use KNN for classification post-dimensionality reduction ● Evaluate the model\n",
        "\n",
        "● Justify this pipeline to your stakeholders as a robust solution for real-world biomedical data\n",
        "\n",
        "(Include your Python code and output in the code box below.)\n",
        "\n",
        "Here’s a complete explanation and Python example for handling a high-dimensional gene expression dataset using PCA + KNN, addressing overfitting and evaluation.\n",
        "\n",
        "1. Pipeline Explanation\n",
        "a. Use PCA to reduce dimensionality\n",
        "High-dimensional data (e.g., thousands of genes) can cause overfitting, especially with few samples.\n",
        "PCA transforms the data into principal components that capture most variance, compressing information while removing noise.\n",
        "b. Decide how many components to keep\n",
        "Compute cumulative explained variance.\n",
        "Choose the minimum number of components that explain a high percentage (e.g., 90–95%) of total variance.\n",
        "c. Use KNN for classification\n",
        "After PCA, KNN can classify patients based on distances in reduced space, avoiding the curse of dimensionality.\n",
        "d. Evaluate the model\n",
        "Use metrics suitable for multi-class classification:\n",
        "\n",
        "Accuracy, F1-score, confusion matrix, and cross-validation.\n",
        "K-fold cross-validation helps ensure robustness on small datasets.\n",
        "\n",
        "e. Justification to stakeholders\n",
        "Reduces noise and dimensionality → less overfitting.\n",
        "PCA ensures critical gene patterns are retained.\n",
        "KNN is simple, interpretable, and non-parametric.\n",
        "Pipeline is scalable and validated with cross-validation, making it suitable for real-world biomedical datasets.\n"
      ],
      "metadata": {
        "id": "Ktc3RVzN6raO"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "2CUC9TZp6nGW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "be9edfc8-e7c5-4d9f-b105-e88432b1b93d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of components to retain 90% variance: 67\n",
            "KNN Accuracy on PCA-reduced data: 0.3500\n",
            "\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.33      0.33      0.33         6\n",
            "           1       0.36      0.71      0.48         7\n",
            "           2       0.00      0.00      0.00         7\n",
            "\n",
            "    accuracy                           0.35        20\n",
            "   macro avg       0.23      0.35      0.27        20\n",
            "weighted avg       0.23      0.35      0.27        20\n",
            "\n",
            "Confusion Matrix:\n",
            "[[2 4 0]\n",
            " [2 5 0]\n",
            " [2 5 0]]\n",
            "5-Fold CV Accuracy: 0.3375 ± 0.1458\n"
          ]
        }
      ],
      "source": [
        "#2. Python Code (Simulated High-Dimensional Data)\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "import numpy as np\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.model_selection import train_test_split, cross_val_score\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "\n",
        "X, y = make_classification(\n",
        "    n_samples=100,\n",
        "    n_features=1000,\n",
        "    n_informative=50,\n",
        "    n_classes=3,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "pca = PCA()\n",
        "X_train_pca = pca.fit_transform(X_train_scaled)\n",
        "X_test_pca = pca.transform(X_test_scaled)\n",
        "\n",
        "cumulative_variance = np.cumsum(pca.explained_variance_ratio_)\n",
        "n_components = np.argmax(cumulative_variance >= 0.90) + 1\n",
        "print(f\"Number of components to retain 90% variance: {n_components}\")\n",
        "\n",
        "pca = PCA(n_components=n_components)\n",
        "X_train_reduced = pca.fit_transform(X_train_scaled)\n",
        "X_test_reduced = pca.transform(X_test_scaled)\n",
        "\n",
        "knn = KNeighborsClassifier(n_neighbors=5)\n",
        "knn.fit(X_train_reduced, y_train)\n",
        "y_pred = knn.predict(X_test_reduced)\n",
        "\n",
        "accuracy = knn.score(X_test_reduced, y_test)\n",
        "print(f\"KNN Accuracy on PCA-reduced data: {accuracy:.4f}\")\n",
        "\n",
        "print(\"\\nClassification Report:\")\n",
        "print(classification_report(y_test, y_pred))\n",
        "\n",
        "print(\"Confusion Matrix:\")\n",
        "print(confusion_matrix(y_test, y_pred))\n",
        "\n",
        "cv_scores = cross_val_score(knn, X_train_reduced, y_train, cv=5)\n",
        "print(f\"5-Fold CV Accuracy: {cv_scores.mean():.4f} ± {cv_scores.std():.4f}\")"
      ]
    }
  ]
}